{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0V4vZ6Qm0ViG"
   },
   "source": [
    "# PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGLrkTFa0ViK"
   },
   "outputs": [],
   "source": [
    "####### CONFIGURATION\n",
    "\n",
    "class CFG:\n",
    "\n",
    "    # environment\n",
    "    environment = 'local'  # work environment ('kaggle', 'colab', 'local')\n",
    "    device      = 'GPU'    # device ('CPU', 'GPU', 'TPU')\n",
    "    num_workers = 4        # no. cores\n",
    "\n",
    "    # general\n",
    "    version  = 131   # notebook version (for saving outputs)\n",
    "    debug    = False  # debug mode runs 5 batches for 2 epochs \n",
    "    training = True  # if False, only inference is run\n",
    "    seed     = 13353 # random state\n",
    "\n",
    "    # data\n",
    "    num_folds  = 5      # number of CV folds\n",
    "    data_2019  = True   # append 2019 labeled data to training folds (1a)\n",
    "    data_pl    = 0.2    # False or percentage of appended pseudo-labeled data (1b)\n",
    "    data_ext   = False  # False or list with external dataset ids (2, 3, 4, 5)\n",
    "    drop_dupl  = True   # drop duplicate images from training folds\n",
    "    drop_outs  = False  # drop outliers from training folds\n",
    "    oversample = False  # enable oversampling through WeightedRandomSampler()\n",
    "    \n",
    "    # label noise\n",
    "    drop_noise  = False  # False or percentage of removed noisy labels in training folds\n",
    "    flip_noise  = False  # False or percentage of flipped noisy labels in training folds\n",
    "\n",
    "    # image size and augmentations \n",
    "    image_size   = 384               # image size after random crop\n",
    "    crop_scale   = (0.10, 1)          # min scale, max scale\n",
    "    gr_shuffle   = (3, 3)             # number of tiles in shuffled grid\n",
    "    ssr          = [0.05, 0.05, 360]  # shift, scale, rotation limits\n",
    "    huesat       = [20, 20, 20]       # hue, saturation, value limits\n",
    "    bricon       = [0.1, 0.1]         # brightness, contrast limits\n",
    "    blur_limit   = 3                  # blur limit\n",
    "    dist_limit   = 0.1                # distortion limit\n",
    "    cutout       = [5, 0.1]           # number of squares, size of squares\n",
    "    clahe        = [0, 1]             # clip limit, size of grid\n",
    "    p_augment    = 0.5                # prob. of augmentations except for flips\n",
    "    cutmix       = [0, 1]             # cutmix batch-level probability, alpha\n",
    "    normalize    = False              # pixel normalization (False, 'dataset', 'imagenet')\n",
    "    pairing      = False              # overlay of a random image from the same class\n",
    "    pairing_prob = 0.6                # probability of an image to get another image added\n",
    "    pixel_crop   = 150                # hight and width of the crop\n",
    "\n",
    "    # model architecture\n",
    "    backbone  = 'vit_base_patch16_384'  # convolutional backbone\n",
    "    weights   = 'imagenet'              # weights ('empty', 'imagenet', 'custom')\n",
    "    attention = False                   # whether to include attention module (efficientnet only)\n",
    "    save_all  = False                   # save weights from each epoch\n",
    "\n",
    "    # pretrained model\n",
    "    pr_version     = 3   # notebook version (2, 3, 4, 5)\n",
    "    pr_num_classes = 10  # no. classes (2: 4, 3: 10, 4: 2, 5: 17)\n",
    "\n",
    "    # training\n",
    "    batch_size  = 16    # no. images per batch\n",
    "    num_epochs  = 10    # no. epochs per fold\n",
    "    fine_tune   = 2     # fine-tuning dense layers (False or no. epochs)\n",
    "    accum_iter  = 2     # no. batches for gradient accumalation\n",
    "    use_amp     = True  # automated mixed precision mode\n",
    "\n",
    "    # learning rate and optimizer\n",
    "    eta     = 1e-4    # starting learning rate\n",
    "    eta_min = 1e-6    # ending learning rate\n",
    "    optim   = 'AdamP' # LR optimizer ('Adam', 'AdamW', 'AdamP')\n",
    "    decay   = 0       # weight decay of optimizer (L2 regularization)\n",
    "\n",
    "    # learning rate scheduler\n",
    "    warmup          = 1                  # no. epochs for warmup\n",
    "    warmup_freeze   = False              # freeze deep layers during warmup\n",
    "    schedule        = 'CosineAnnealing'  # LR scheduler after warmup\n",
    "    update_on_batch = True               # update LR after every batch or epoch\n",
    "\n",
    "    # loss function\n",
    "    loss_fn     = 'OHEM'         # loss function ('CE', 'OHEM', 'SCE', 'CCE', 'Focal', 'FocalCosine', 'Taylor', 'BiTempered')\n",
    "    smoothing   = 0.2          # label smoothing (works with all losses)\n",
    "    ohem        = 0.8          # OHEM loss parameters: top-k percentage\n",
    "    sce         = [0.1, 1.0]   # SCE loss parameters: alpha, beta\n",
    "    cce         = 5            # CCE loss parameters: gamma\n",
    "    focal       = [1, 2]       # Focal loss parameters: alpha, gamma\n",
    "    focalcosine = [1, 2, 0.1]  # FocalCosine loss parameters: alpha, gamma, xent\n",
    "    taylor      = 2            # Taylor loss parameters: n\n",
    "    bitempered  = [0.3, 1.0]   # BiTempered loss parameters: t1, t2\n",
    "\n",
    "    # epoch-based changes\n",
    "    step_size  = False  # False or list with image_size multiplier for each epoch\n",
    "    step_class = False  # False or list with num_classes for each epoch (2 or 5)\n",
    "    step_p_aug = False  # False or list with p_augment multiplier for each epoch\n",
    "    step_loss  = False  # False or list with loss functions for each epoch\n",
    "\n",
    "    # inference\n",
    "    num_tta = 1  # no. TTA flips (between 1 and 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONx3m0TJ2PJ1"
   },
   "outputs": [],
   "source": [
    "####### CONVERT CONFIGURATION\n",
    "\n",
    "CFG = dict(vars(CFG))\n",
    "for key in ['__dict__', '__doc__', '__module__', '__weakref__']:\n",
    "    del CFG[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-9ZKt08y0ViT"
   },
   "source": [
    "# PREPARATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nAfc6niM0ep4",
    "outputId": "0397bcd7-7ff4-4809-be9d-f81170eaf814"
   },
   "outputs": [],
   "source": [
    "####### ENVIRONMENT SETUP\n",
    "\n",
    "##### COLAB\n",
    "\n",
    "import os\n",
    "if (CFG['environment'] == 'colab') and (not os.path.exists('/content/cassava/')):\n",
    "\n",
    "    # mount drive\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')  \n",
    "\n",
    "    # set up Kaggle API\n",
    "    !pip install --upgrade --force-reinstall --no-deps -q kaggle\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp '/content/drive/MyDrive/Competitions/kaggle.json' ~/.kaggle/\n",
    "    !ls ~/.kaggle\n",
    "\n",
    "    # download data\n",
    "    !mkdir '/content/cassava/'\n",
    "    !kaggle kernels output kozodoi/merge-and-zip-2019-2020-data -p '/content/cassava/'\n",
    "    !unzip -o -q '/content/cassava/images.zip' -d '/content/cassava/'\n",
    "    !rm -rf '/content/cassava/images.zip'\n",
    "    \n",
    "    \n",
    "##### KAGGLE\n",
    "\n",
    "if (CFG['environment'] == 'kaggle') and (not os.path.exists('cassava/')):\n",
    "\n",
    "    # extract data\n",
    "    !mkdir 'cassava/'\n",
    "    !unzip -o -q '/kaggle/input/merge-and-zip-2019-2020-data/images.zip' -d 'cassava/'\n",
    "    !cp '/kaggle/input/merge-and-zip-2019-2020-data/train.csv' 'cassava/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2_X93RyNgsJI"
   },
   "outputs": [],
   "source": [
    "####### WORKING PATHS\n",
    "\n",
    "# kaggle\n",
    "if CFG['environment'] == 'kaggle':\n",
    "    CFG['data_path']  = 'cassava/'\n",
    "    CFG['model_path'] = ''\n",
    "    CFG['out_path']   = ''\n",
    "\n",
    "# colab\n",
    "if CFG['environment'] == 'colab':\n",
    "    CFG['data_path']  = '/content/cassava/'\n",
    "    CFG['model_path'] = '/content/drive/MyDrive/Cassava/pretraining/v' + str(CFG['pr_version']) + '/'\n",
    "    CFG['out_path']   = '/content/drive/MyDrive/Cassava/output/v'      + str(CFG['version'])    + '/'\n",
    "    if not os.path.exists(CFG['out_path']):\n",
    "        os.mkdir(CFG['out_path'])\n",
    "\n",
    "# local\n",
    "if CFG['environment'] == 'local':\n",
    "    CFG['data_path']  = '../../data/'\n",
    "    CFG['model_path'] = '../../pretraining/v' + str(CFG['pr_version']) + '/'\n",
    "    CFG['out_path']   = '../../output/v'      + str(CFG['version'])    + '/'\n",
    "    if not os.path.exists(CFG['out_path']):\n",
    "        os.mkdir(CFG['out_path'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HIjTLhkSBRbe"
   },
   "outputs": [],
   "source": [
    "####### PACKAGES FOR TPU\n",
    "\n",
    "if CFG['device'] == 'TPU':\n",
    "        \n",
    "    # install XLA\n",
    "    xla_version = 'nightly' # (1.7, 'nightly')\n",
    "    !curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "    !python pytorch-xla-env-setup.py --version $xla_version\n",
    "\n",
    "    # settings\n",
    "    if CFG['environment'] == 'colab':\n",
    "        import os\n",
    "        assert os.environ['COLAB_TPU_ADDR']\n",
    "    os.environ['XLA_USE_BF_16'] = '1'\n",
    "    os.environ['XLA_TENSOR_ALLOCATOR_MAXSIZE'] = '100000000'\n",
    "\n",
    "    # imports\n",
    "    import torch_xla\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "    import torch_xla.distributed.parallel_loader as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S6rE_WKH0ViV",
    "outputId": "f89011cf-9e06-4ee5-9705-ac153535284a"
   },
   "outputs": [],
   "source": [
    "####### PACKAGES\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import RandomSampler, SequentialSampler, WeightedRandomSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, ReduceLROnPlateau\n",
    "from torch.autograd import Function\n",
    "\n",
    "!pip install timm\n",
    "import timm\n",
    "from timm.utils import *\n",
    "\n",
    "from contextlib import suppress\n",
    "\n",
    "!pip install --upgrade -U albumentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "!pip install git+'https://github.com/ildoonet/pytorch-gradual-warmup-lr.git'\n",
    "from warmup_scheduler import GradualWarmupScheduler  \n",
    "\n",
    "!pip install adamp\n",
    "from adamp import AdamP\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import cv2\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from scipy.special import softmax\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### HELPER FUNCTIONS\n",
    "\n",
    "sys.path.append('../../codes')\n",
    "\n",
    "# utilities\n",
    "from utilities import *\n",
    "\n",
    "# data processing\n",
    "from data import LeafData, get_data\n",
    "from augmentations import get_augs, rand_bbox, cutmix_fn, get_tta_flips\n",
    "\n",
    "# losses\n",
    "from losses import *\n",
    "\n",
    "# model\n",
    "from model import *\n",
    "\n",
    "# training and inference\n",
    "from train_epoch import train_epoch\n",
    "from valid_epoch import valid_epoch\n",
    "from run_fold import run_fold\n",
    "from plot_results import plot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwoqOvA10Vim",
    "outputId": "dfa9e699-b233-49ee-b195-6021d0ba1eb4"
   },
   "outputs": [],
   "source": [
    "####### TRAINING DEVICE\n",
    "\n",
    "if CFG['device'] == 'TPU':\n",
    "    print('Training on TPU...')\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "if CFG['device'] == 'GPU':\n",
    "    print('Training on GPU...')\n",
    "    if CFG['environment'] == 'local':\n",
    "        device = torch.device('cuda:1')\n",
    "    else:\n",
    "        device = torch.device('cuda:0')\n",
    "\n",
    "if CFG['device'] == 'CPU':\n",
    "    print('Training on CPU...')\n",
    "    device = torch.device('cpu') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2OuAGJw0Viv"
   },
   "source": [
    "# DATA PREP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "70WIWDTE0Vi8",
    "outputId": "2cad2e1a-70d4-45d1-d35b-b3748575df8d"
   },
   "outputs": [],
   "source": [
    "####### 2020 COMPETITION DATA\n",
    "\n",
    "# import data\n",
    "df = pd.read_csv(CFG['data_path'] + 'train.csv')\n",
    "df = df.loc[df['source'] == 2020].reset_index(drop = True)\n",
    "    \n",
    "# num classes\n",
    "CFG['num_classes'] = df['label'].nunique()\n",
    "\n",
    "# partitioning\n",
    "df['fold'] = -1\n",
    "skf = StratifiedKFold(n_splits = CFG['num_folds'], random_state = CFG['seed'], shuffle = True)\n",
    "for fold, (trn_idx, val_idx) in enumerate(skf.split(df, df['label'])):\n",
    "    df.loc[val_idx, 'fold'] = fold\n",
    "    \n",
    "# check folds\n",
    "pd.crosstab(df['fold'], df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "0m1cX4ACuBxn",
    "outputId": "0e3e56bf-33e5-4322-9c78-cd506d762419"
   },
   "outputs": [],
   "source": [
    "####### 2019 COMPETITION DATA\n",
    "\n",
    "##### LABELED DATA\n",
    "\n",
    "if CFG['data_2019']:\n",
    "\n",
    "    # import data\n",
    "    df_2019 = pd.read_csv(CFG['data_path'] + 'train.csv')\n",
    "    df_2019 = df_2019.loc[df_2019['source'] == 2019].reset_index(drop = True)\n",
    "    df_2019['fold'] = -1\n",
    "        \n",
    "    # check classes\n",
    "    display(df_2019['label'].value_counts())\n",
    "\n",
    "\n",
    "##### PSEUDO-LABELED DATA\n",
    "\n",
    "if CFG['data_pl']:\n",
    "\n",
    "    # import\n",
    "    if CFG['environment'] == 'colab':\n",
    "        !kaggle kernels output kozodoi/pseudo-labeling-v57 -p '/content/cassava/'\n",
    "        !unzip -o -q '/content/cassava/unlabeled_images.zip' -d '/content/cassava/train_images/'\n",
    "        !rm -rf '/content/cassava/unlabeled_images.zip'\n",
    "    df_pl = pd.read_csv(CFG['data_path'] + 'df_pl.csv')\n",
    "    df_pl['fold'] = -1\n",
    "\n",
    "    # filter confident predictions\n",
    "    df_pl['confidence'] = df_pl.filter(like = 'c').max(axis = 1)\n",
    "    df_pl = df_pl.sort_values('confidence', ascending = False)\n",
    "    df_pl = df_pl.head(int(CFG['data_pl'] * len(df_pl)))\n",
    "    df_pl = df_pl[['image_id', 'label', 'fold']]\n",
    "\n",
    "    # check classes\n",
    "    display(df_pl['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvG_0mWICb-U"
   },
   "outputs": [],
   "source": [
    "####### EXTERNAL DATA [HEALTHY]\n",
    "\n",
    "if CFG['data_ext']:\n",
    "\n",
    "    # placeholder\n",
    "    df_ext = pd.DataFrame(columns = ['image_id', 'label', 'source'])\n",
    "\n",
    "\n",
    "    ##### DATASET 2\n",
    "\n",
    "    if 2 in CFG['data_ext']:\n",
    "\n",
    "        # import images\n",
    "        !kaggle kernels output kozodoi/prepare-dataset-2 -p '/content/cassava/'\n",
    "        !unzip -o -q '/content/cassava/images.zip' -d '/content/cassava/train_images/'\n",
    "        !rm -rf '/content/cassava/images.zip'\n",
    "\n",
    "        # filtering\n",
    "        df_2 = pd.read_csv(CFG['data_path'] + 'df_2.csv')\n",
    "        df_2 = df_2.loc[df_2['label'] == 4].reset_index(drop = True)\n",
    "        df_2['source'] = 2\n",
    "        df_ext = pd.concat([df_ext, df_2], axis = 0).reset_index(drop = True)\n",
    "        \n",
    "\n",
    "    ##### DATASET 3\n",
    "\n",
    "    if 3 in CFG['data_ext']:\n",
    "\n",
    "        # import\n",
    "        !kaggle kernels output kozodoi/prepare-dataset-3 -p '/content/cassava/'\n",
    "        !unzip -o -q '/content/cassava/images.zip' -d '/content/cassava/train_images/'\n",
    "        !rm -rf '/content/cassava/images.zip'\n",
    "\n",
    "        # filtering\n",
    "        df_3 = pd.read_csv(CFG['data_path'] + 'df_3.csv')\n",
    "        df_3 = df_3.loc[df_3['label'] == 4].reset_index(drop = True)\n",
    "        df_3['source'] = 3\n",
    "        df_ext = pd.concat([df_ext, df_3], axis = 0).reset_index(drop = True)\n",
    "\n",
    "\n",
    "    ##### DATASET 4\n",
    "\n",
    "    if 4 in CFG['data_ext']:\n",
    "\n",
    "        # import\n",
    "        !kaggle kernels output kozodoi/prepare-dataset-4 -p '/content/cassava/'\n",
    "        !unzip -o -q '/content/cassava/images.zip' -d '/content/cassava/train_images/'\n",
    "        !rm -rf '/content/cassava/images.zip'\n",
    "\n",
    "        # filtering\n",
    "        df_4 = pd.read_csv(CFG['data_path'] + 'df_4.csv')\n",
    "        df_4 = df_4.loc[df_4['label'] == 4].reset_index(drop = True)\n",
    "        df_4['source'] = 4\n",
    "        df_ext = pd.concat([df_ext, df_4], axis = 0).reset_index(drop = True)\n",
    "\n",
    "\n",
    "    ##### DATASET 5\n",
    "\n",
    "    if 5 in CFG['data_ext']:\n",
    "\n",
    "        # import\n",
    "        !kaggle kernels output kozodoi/prepare-dataset-5 -p '/content/cassava/'\n",
    "        !unzip -o -q '/content/cassava/images.zip' -d '/content/cassava/train_images/'\n",
    "        !rm -rf '/content/cassava/images.zip'\n",
    "\n",
    "        # filtering\n",
    "        df_5 = pd.read_csv(CFG['data_path'] + 'df_5.csv')\n",
    "        df_5 = df_5.loc[df_5['label'] == 4].reset_index(drop = True)\n",
    "        df_5['source'] = 5\n",
    "        df_ext = pd.concat([df_ext, df_5], axis = 0).reset_index(drop = True)\n",
    "\n",
    "\n",
    "    ##### PARTITIONING\n",
    "\n",
    "    df_ext['source'] = df_ext['source'].astype('int')\n",
    "    df_ext['fold']   = -1\n",
    "    display(df_ext['source'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bo5lww9Jpim1",
    "outputId": "26c2936f-e6ec-4331-db74-7506e2a51b1f"
   },
   "outputs": [],
   "source": [
    "####### IDENTIFY BAD EXAMPLES\n",
    "\n",
    "##### DUPLICATES\n",
    "\n",
    "if CFG['drop_dupl']:\n",
    "\n",
    "    # duplicates: all data with DBSCAN\n",
    "    duplicates   = [['extra-image-1540.jpg', '3286430972.jpg'], ['extra-image-5393.jpg', '1007196516.jpg'], ['extra-image-9233.jpg', '177429020.jpg'], ['extra-image-11388.jpg', '401103417.jpg'], ['extra-image-4308.jpg', '2293657925.jpg'], ['train-cbsd-216.jpg', 'train-cbb-381.jpg', 'test-img-1450.jpg'], ['train-cgm-90.jpg', 'train-cbsd-791.jpg', 'test-img-2051.jpg'], ['extra-image-1924.jpg', '1710187645.jpg'], ['extra-image-5502.jpg', '3513881408.jpg'], ['extra-image-2527.jpg', '4080972605.jpg'], ['extra-image-5475.jpg', '2309908274.jpg'], ['extra-image-428.jpg', '490262929.jpg'], ['train-cgm-373.jpg', 'test-img-3468.jpg'], ['train-cbsd-123.jpg', 'test-img-1610.jpg'], ['extra-image-549.jpg', '2259763259.jpg'], ['extra-image-6922.jpg', 'extra-image-12476.jpg', '825340900.jpg'], ['extra-image-2618.jpg', '1318419572.jpg'], ['extra-image-7208.jpg', 'extra-image-109.jpg', '1273363177.jpg'], ['extra-image-11114.jpg', '1618418212.jpg'], ['test-img-1455.jpg', '4016154109.jpg'], ['extra-image-563.jpg', '3789300725.jpg'], ['train-cmd-2506.jpg', 'train-cmd-1997.jpg', 'test-img-1792.jpg'], ['extra-image-15978.jpg', '1720773250.jpg'], ['train-cmd-366.jpg', 'test-img-3080.jpg'], ['extra-image-2732.jpg', '490603548.jpg'], ['extra-image-14790.jpg', 'extra-image-12199.jpg'], ['train-cmd-900.jpg', '2651363651.jpg'], ['extra-image-933.jpg', '479472063.jpg'], ['extra-image-6992.jpg', '3676057452.jpg'], ['532464272.jpg', '3361673129.jpg'], ['extra-image-10457.jpg', '3118293114.jpg'], ['extra-image-633.jpg', '3069603925.jpg'], ['train-cgm-563.jpg', '4110644267.jpg'], ['extra-image-1372.jpg', '1466391339.jpg'], ['train-cbb-282.jpg', '3476081387.jpg'], ['extra-image-4289.jpg', '2333536768.jpg'], ['extra-image-9788.jpg', '1558894441.jpg'], ['extra-image-9728.jpg', '2704051934.jpg'], ['3507045403.jpg', '2243541656.jpg'], ['extra-image-9779.jpg', '1719304067.jpg'], ['extra-image-3139.jpg', '3361179403.jpg'], ['extra-image-15471.jpg', '1621255319.jpg'], ['extra-image-11326.jpg', '3647124621.jpg'], ['extra-image-6559.jpg', '2519536403.jpg'], ['extra-image-14943.jpg', '952303505.jpg'], ['train-cbsd-1150.jpg', '867127390.jpg'], ['extra-image-5923.jpg', '1091727259.jpg'], ['train-cmd-2170.jpg', '3903538298.jpg'], ['extra-image-12924.jpg', '1131959133.jpg'], ['extra-image-13477.jpg', '2904848270.jpg'], ['extra-image-3871.jpg', '4059169921.jpg'], ['extra-image-4513.jpg', '3662840429.jpg'], ['extra-image-4605.jpg', '232710623.jpg'], ['extra-image-9217.jpg', '1200108798.jpg'], ['extra-image-2419.jpg', '690440568.jpg', '3803823261.jpg'], ['extra-image-12887.jpg', '1989789271.jpg'], ['3251960666.jpg', '2925605732.jpg', '1770746162.jpg'], ['extra-image-4112.jpg', '800250438.jpg'], ['extra-image-10326.jpg', '2888967355.jpg'], ['train-cmd-2615.jpg', 'test-img-3118.jpg'], ['test-img-2592.jpg', '943110824.jpg'], ['extra-image-9616.jpg', '1332855741.jpg'], ['test-img-998.jpg', '58720038.jpg'], ['extra-image-5076.jpg', '2801552324.jpg'], ['extra-image-9334.jpg', '4245562546.jpg'], ['extra-image-12355.jpg', '185236448.jpg'], ['extra-image-5576.jpg', '1761795923.jpg'], ['extra-image-7798.jpg', '3148313410.jpg'], ['train-cgm-8.jpg', '3859028489.jpg'], ['extra-image-13482.jpg', '426528709.jpg'], ['extra-image-12337.jpg', '4089645917.jpg'], ['extra-image-6633.jpg', '2437547294.jpg'], ['extra-image-2857.jpg', '2246452498.jpg'], ['extra-image-590.jpg', '3677086623.jpg'], ['extra-image-10042.jpg', '169189292.jpg'], ['extra-image-8920.jpg', 'extra-image-2601.jpg'], ['extra-image-4990.jpg', '4138769021.jpg'], ['train-cbsd-1437.jpg', 'test-img-3607.jpg', 'test-img-2634.jpg'], ['train-cmd-2459.jpg', 'test-img-698.jpg', 'test-img-3742.jpg', 'test-img-2354.jpg'], ['test-img-1194.jpg', '1007700625.jpg'], ['extra-image-5652.jpg', '3418761424.jpg'], ['extra-image-12886.jpg', '1529177483.jpg'], ['588169966.jpg', '1733354827.jpg'], ['train-cmd-2102.jpg', 'train-cmd-2045.jpg', 'train-cmd-1076.jpg', 'test-img-183.jpg'], ['test-img-3037.jpg', '2760272941.jpg'], ['extra-image-6996.jpg', '3258193135.jpg'], ['extra-image-10309.jpg', '19872828.jpg', '1796762442.jpg'], ['train-cmd-790.jpg', 'train-cmd-2355.jpg'], ['extra-image-9742.jpg', '3132128369.jpg'], ['extra-image-7929.jpg', '2828723686.jpg'], ['extra-image-14835.jpg', '744619434.jpg'], ['extra-image-8184.jpg', '3198115498.jpg'], ['extra-image-15188.jpg', '239700501.jpg'], ['extra-image-3641.jpg', '1694968228.jpg'], ['extra-image-4976.jpg', '1660551148.jpg'], ['test-img-1767.jpg', 'test-img-1219.jpg'], ['extra-image-2468.jpg', '4195598174.jpg'], ['extra-image-12556.jpg', '3889376143.jpg'], ['extra-image-1563.jpg', '1803055979.jpg'], ['extra-image-2068.jpg', '1089759212.jpg'], ['extra-image-8657.jpg', '3055297068.jpg'], ['extra-image-5237.jpg', '245076718.jpg'], ['extra-image-11168.jpg', '3354494955.jpg'], ['extra-image-12603.jpg', '4134659314.jpg'], ['extra-image-16033.jpg', '597389720.jpg'], ['extra-image-14012.jpg', '4234497061.jpg'], ['test-img-138.jpg', '3884189662.jpg'], ['extra-image-1992.jpg', '1982702143.jpg'], ['extra-image-5798.jpg', '3048105792.jpg'], ['test-img-439.jpg', '2259667328.jpg'], ['test-img-1145.jpg', '1356659344.jpg'], ['extra-image-9462.jpg', '3826670209.jpg'], ['extra-image-456.jpg', '945966296.jpg'], ['train-cbb-129.jpg', '519050764.jpg'], ['extra-image-12241.jpg', '2876852928.jpg'], ['extra-image-13234.jpg', '1973109559.jpg'], ['extra-image-6232.jpg', '2659874728.jpg'], ['extra-image-3883.jpg', '2086525919.jpg'], ['extra-image-12735.jpg', '1263625799.jpg'], ['extra-image-2704.jpg', 'extra-image-12619.jpg', '534364866.jpg'], ['extra-image-7008.jpg', '459566440.jpg'], ['extra-image-771.jpg', '1893842412.jpg'], ['extra-image-4438.jpg', '3036253841.jpg'], ['extra-image-6310.jpg', '1663857014.jpg'], ['extra-image-10771.jpg', '1435171668.jpg'], ['extra-image-6392.jpg', '2305895487.jpg'], ['extra-image-8649.jpg', '1831389246.jpg'], ['extra-image-11953.jpg', '1043857809.jpg'], ['test-img-242.jpg', 'test-img-2340.jpg'], ['extra-image-2434.jpg', '234486821.jpg'], ['extra-image-11518.jpg', '1465774459.jpg'], ['extra-image-2589.jpg', '671675016.jpg'], ['extra-image-11209.jpg', '3051541520.jpg'], ['extra-image-7346.jpg', '4070631340.jpg'], ['extra-image-652.jpg', '3746679490.jpg'], ['extra-image-14165.jpg', '2008776850.jpg'], ['extra-image-8273.jpg', '2181262010.jpg'], ['extra-image-6930.jpg', '3649693860.jpg'], ['test-img-3350.jpg', '3709602808.jpg'], ['extra-image-14435.jpg', '2976631036.jpg'], ['extra-image-1152.jpg', '3304610004.jpg'], ['extra-image-4709.jpg', '2810787386.jpg'], ['train-cmd-1507.jpg', 'test-img-33.jpg', 'test-img-235.jpg'], ['extra-image-15422.jpg', '1381973639.jpg'], ['extra-image-2259.jpg', '1061187521.jpg'], ['extra-image-9075.jpg', '2356938780.jpg'], ['extra-image-4587.jpg', '1160018036.jpg'], ['extra-image-14543.jpg', '138109251.jpg'], ['extra-image-9167.jpg', '2583894106.jpg'], ['extra-image-4718.jpg', '1297475127.jpg'], ['extra-image-14501.jpg', '2435784137.jpg'], ['extra-image-13365.jpg', '2126921679.jpg'], ['extra-image-3186.jpg', '324248837.jpg'], ['extra-image-3175.jpg', '1820923810.jpg'], ['421035788.jpg', '1008244905.jpg'], ['extra-image-8581.jpg', '4153074724.jpg'], ['extra-image-5561.jpg', '408051106.jpg'], ['extra-image-6928.jpg', '2396550881.jpg'], ['extra-image-9250.jpg', '3407709856.jpg'], ['train-cbsd-251.jpg', '3001382345.jpg'], ['extra-image-15574.jpg', '726377415.jpg']]\n",
    "    false_images = ['train-cmd-2506.jpg', 'extra-image-14790.jpg', '53246272.jpg', '3507045403.jpg', 'test-img-3607.jpg', '19872828.jpg', 'extra-image-2619.jpg']\n",
    "    \n",
    "    # convert to duplicate list\n",
    "    list_dupl = []\n",
    "    for row in duplicates:\n",
    "        list_dupl += row[:-1]\n",
    "    list_dupl = list(set(list_dupl))\n",
    "    list_dupl = [l for l in list_dupl if l not in false_images]\n",
    "    del duplicates\n",
    "    print('No. duplicate images:', len(list_dupl))\n",
    "\n",
    "\n",
    "##### OUTLIERS\n",
    "\n",
    "if CFG['drop_outs']:\n",
    "\n",
    "    list_outs = ['1033403106.jpg', '104535906.jpg', '1084966463.jpg', '1200108798.jpg', '1230982659.jpg', '1234375577.jpg', '1329083657.jpg', '133263895.jpg', '1395513159.jpg', '1541808301.jpg', '1576487452.jpg', '1635136764.jpg', '1689108113.jpg', '1765374655.jpg', \n",
    "                 '1841279687.jpg', '1870791731.jpg', '1894675387.jpg', '2052899282.jpg', '2151050324.jpg', '2298308938.jpg', '2321669192.jpg', '2378878506.jpg', '2504430507.jpg', '2642216511.jpg', '2666462236.jpg', '2719114674.jpg', '2739645903.jpg', '2786559298.jpg', \n",
    "                 '2853279464.jpg', '2889965946.jpg', '2967206024.jpg', '2968477250.jpg', '3002089936.jpg', '3212523761.jpg', '3238801760.jpg', '3250352507.jpg', '3267697264.jpg', '3433580973.jpg', '3806787164.jpg', '4196928486.jpg', '4250554784.jpg', '456647345.jpg', \n",
    "                 '479609263.jpg', '511494516.jpg', '534270890.jpg', '587829607.jpg', '65139094.jpg', '690440568.jpg', '74966012.jpg', '854773586.jpg', 'train-cmd-328.jpg', 'train-cbsd-319.jpg', 'train-cgm-223.jpg', 'train-cgm-190.jpg', 'train-cmd-1479.jpg']\n",
    "    print('No. outlier images:', len(list_outs))\n",
    "\n",
    "\n",
    "##### NOISY IMAGES\n",
    "\n",
    "if (CFG['drop_noise']) or (CFG['flip_noise']):\n",
    "\n",
    "    # import\n",
    "    df_no = pd.read_csv(CFG['data_path'] + 'oof_median.csv')\n",
    "\n",
    "    # filter confident predictions\n",
    "    df_no['pred']      = np.argmax(df_no[['c0', 'c1', 'c2', 'c3', 'c4']].values, axis = 1)\n",
    "    df_no['confidence'] = df_no[['c0', 'c1', 'c2', 'c3', 'c4']].max(axis = 1)\n",
    "    df_no = df_no.loc[df_no['pred'] != df_no['label']].reset_index(drop = True)\n",
    "\n",
    "    # extract top wrong preds per class\n",
    "    df_no = df_no.sort_values(['label', 'confidence'], ascending = False)\n",
    "    top_p = CFG['droop_noise'] if CFG['drop_noise'] else CFG['flip_noise']\n",
    "    df_no = df_no.groupby('label').apply(lambda x: x.head(int(len(x)*top_p))).reset_index(drop = True)\n",
    "    print('No. noisy images:', len(df_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cgwYKa_OkwFs"
   },
   "outputs": [],
   "source": [
    "####### DATASET\n",
    "\n",
    "class LeafData(Dataset):\n",
    "    \n",
    "    # initialization\n",
    "    def __init__(self, \n",
    "                 data, \n",
    "                 directory, \n",
    "                 transform    = None, \n",
    "                 labeled      = False,\n",
    "                 pairing      = False,\n",
    "                 pairing_prob = 0.01,\n",
    "                 pixel_crop   = 50,\n",
    "                 seed         = 0):\n",
    "        \n",
    "        self.data         = data\n",
    "        self.directory    = directory\n",
    "        self.transform    = transform\n",
    "        self.labeled      = labeled\n",
    "        self.pairing      = pairing\n",
    "        self.pairing_prob = pairing_prob\n",
    "        self.seed         = seed\n",
    "        self.counter      = 0\n",
    "        self.pixel_crop   = pixel_crop\n",
    "        if pairing:\n",
    "            self.sampled_idx = self.data.sample(frac=self.pairing_prob, random_state=self.seed).index\n",
    "        \n",
    "    # length\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # get item  \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        # import\n",
    "        path  = os.path.join(self.directory, self.data.iloc[idx]['image_id'])\n",
    "        image = cv2.imread(path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "        # augmentations\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image = image)['image']\n",
    "            \n",
    "        if self.pairing:\n",
    "            if idx in self.sampled_idx:\n",
    "                \n",
    "                try:\n",
    "                    add_path     = os.path.join(self.directory, self.data.loc[self.data.label==self.data.iloc[idx]['label']].sample(1, random_state=self.seed)['image_id'].iloc[0])\n",
    "                except:\n",
    "                    print(self.data.sample(1, random_state=self.seed))\n",
    "                    add_path     = os.path.join(self.directory, self.data.sample(1, random_state=self.seed)['image_id'].iloc[0])\n",
    "                    self.counter += 1\n",
    "                    \n",
    "                add_image = cv2.imread(add_path)\n",
    "                add_image = cv2.cvtColor(add_image, cv2.COLOR_BGR2RGB)\n",
    "                if self.transform is not None:\n",
    "                    add_image = self.transform(image = add_image)['image']\n",
    "                    \n",
    "                max_x = add_image.shape[2] - self.pixel_crop\n",
    "                max_y = add_image.shape[1] - self.pixel_crop\n",
    "                \n",
    "                x = torch.randint(0, max_x, (1,))\n",
    "                y = torch.randint(0, max_y, (1,))\n",
    "                crop  = add_image[:, y: y + self.pixel_crop, x: x + self.pixel_crop]\n",
    "                image[:, y: y + self.pixel_crop, x: x + self.pixel_crop] = crop\n",
    "                \n",
    "        \n",
    "        # output\n",
    "        if self.labeled:\n",
    "            labels = torch.tensor(self.data.iloc[idx]['label']).long()\n",
    "            return image, labels\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "        def get_counter(self):\n",
    "            print(f'{self.counter} observations were cropped with the sample from another class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "xBXNJJBe3uw_",
    "outputId": "21268877-6803-427d-e339-7bb181dabb93"
   },
   "outputs": [],
   "source": [
    "####### EXAMINE SAMPLE BATCH\n",
    "\n",
    "# sample size\n",
    "sample = 5\n",
    "\n",
    "# augmentations\n",
    "train_augs, test_augs = get_augs(CFG, image_size = 128)\n",
    "\n",
    "# datasets\n",
    "train_dataset = LeafData(data         = df.head(sample*2), \n",
    "                         directory    = CFG['data_path'] + 'train_images/',\n",
    "                         transform    = train_augs,\n",
    "                         labeled      = True, \n",
    "                         pairing      = CFG['pairing'],\n",
    "                         pairing_prob = 0.3,\n",
    "                         seed         = CFG['seed'])\n",
    "test_dataset = LeafData(data       = df.head(sample*2), \n",
    "                        directory  = CFG['data_path'] + 'train_images/',\n",
    "                        transform  = test_augs,\n",
    "                        labeled    = True)\n",
    "\n",
    "# data loaders\n",
    "train_loader = DataLoader(dataset     = train_dataset, \n",
    "                          batch_size  = sample, \n",
    "                          shuffle       = False, \n",
    "                          num_workers = 0,\n",
    "                          pin_memory  = True)\n",
    "test_loader = DataLoader(dataset      = test_dataset, \n",
    "                         batch_size   = sample, \n",
    "                         shuffle        = False, \n",
    "                         num_workers  = 0,\n",
    "                         pin_memory   = True)\n",
    "\n",
    "# display train images\n",
    "batch_time = time.time()\n",
    "for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "\n",
    "    # apply cutmix augmentation\n",
    "    if CFG['cutmix'][0] > 0:\n",
    "        mix_decision = 0 #np.random.rand(1)\n",
    "        if mix_decision < CFG['cutmix'][0]:\n",
    "            inputs, _ = cutmix_fn(data   = inputs, \n",
    "                                  target = labels, \n",
    "                                  alpha  = cutmix[1])\n",
    "\n",
    "    # feedback\n",
    "    inputs_shape = inputs.shape\n",
    "    load_time    = time.time() - batch_time\n",
    "    pixel_values = [torch.min(inputs).item(), torch.mean(inputs).item(), torch.max(inputs).item()]\n",
    "\n",
    "    # examples\n",
    "    fig = plt.figure(figsize = (20, 8))\n",
    "    for i in range(sample):\n",
    "        ax = fig.add_subplot(2, sample, i + 1, xticks = [], yticks = [])     \n",
    "        plt.imshow(inputs[i].numpy().transpose(1, 2, 0))\n",
    "        ax.set_title('Label: {} [train]'.format(labels[i].numpy()), color = 'red')\n",
    "    break\n",
    "\n",
    "# display test images\n",
    "batch_time = time.time()\n",
    "for batch_idx, (inputs, labels) in enumerate(test_loader):\n",
    "\n",
    "    # feedback\n",
    "    print('- batch shape:  {} vs {}'.format(inputs_shape, inputs.shape))\n",
    "    print('- loading time: {:.4f} vs {:.4f} seconds'.format(load_time, (time.time() - batch_time)))\n",
    "    print('- pixel values: {:.2f} - {:.2f} - {:.2f} vs {:.2f} - {:.2f} - {:.2f}'.format(\n",
    "        pixel_values[0], pixel_values[1], pixel_values[2],\n",
    "        torch.min(inputs).item(), torch.mean(inputs).item(), torch.max(inputs).item()))\n",
    "\n",
    "    # examples\n",
    "    for i in range(sample):\n",
    "        ax = fig.add_subplot(2, sample, sample + i + 1, xticks = [], yticks = [])     \n",
    "        plt.imshow(inputs[i].numpy().transpose(1, 2, 0))\n",
    "        ax.set_title('Label: {} [valid]'.format(labels[i].numpy()), color = 'green')\n",
    "    plt.savefig(CFG['out_path'] + 'fig_sample.png')\n",
    "    break\n",
    "    \n",
    "# clean up\n",
    "del inputs, labels, batch_idx, train_loader, test_loader, train_dataset, test_dataset, train_augs, test_augs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NtAEmeT80VjI"
   },
   "source": [
    "# CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 887
    },
    "id": "VIOZZY6cfxBx",
    "outputId": "c650471a-505c-476c-f671-046a56f39d2a"
   },
   "outputs": [],
   "source": [
    "####### CROSS-VALIDATION LOOP\n",
    "\n",
    "##### INITIALIZATION\n",
    "\n",
    "# timer\n",
    "cv_start = time.time()\n",
    "\n",
    "# clear memory\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "# placeholders\n",
    "oof = None\n",
    "perf = pd.DataFrame(columns = ['fold', 'epoch', 'trn_loss', 'val_loss', 'val_acc'])\n",
    "if not CFG['training']:\n",
    "    perf = pd.read_csv(CFG['out_path'] + 'tab_performance.csv')\n",
    "\n",
    "# amp settings\n",
    "amp_autocast = suppress\n",
    "if CFG['use_amp']:\n",
    "    amp_autocast = torch.cuda.amp.autocast\n",
    "    scaler       = torch.cuda.amp.GradScaler()       \n",
    "\n",
    "# adjust epochs if needed\n",
    "if not CFG['training']:\n",
    "    CFG['num_epochs'] = 1\n",
    "    CFG['fine_tune']   = False\n",
    "if CFG['debug']:\n",
    "    CFG['num_epochs'] = 2\n",
    "    CFG['num_folds']  = 2\n",
    "    CFG['fine_tune']   = 1\n",
    "    \n",
    "    \n",
    "##### CROSS-VALIDAION\n",
    "\n",
    "for fold in range(CFG['num_folds']):\n",
    "    \n",
    "    ### PERFORM MODELING\n",
    "    \n",
    "    # feedback\n",
    "    print('-' * 55)\n",
    "    print('FOLD {:d}/{:d}'.format(fold + 1, CFG['num_folds']))    \n",
    "    print('-' * 55) \n",
    "    \n",
    "    # prepare data\n",
    "    trn_loader, val_loader, df_trn, df_val = get_data(df, fold, CFG,\n",
    "                                                      df_2019    = df_2019 if CFG['data_2019'] else None,\n",
    "                                                      df_pl      = df_pl if CFG['data_pl'] else None,\n",
    "                                                      df_ext     = df_ext if CFG['data_ext'] else None,\n",
    "                                                      df_no      = df_no,\n",
    "                                                      list_dupl  = list_dupl if CFG['drop_dupl'] else [],\n",
    "                                                      list_noise = list_noise if CFG['drop_noise'] else []) \n",
    "\n",
    "    # training and inference\n",
    "    if CFG['device'] != 'TPU':\n",
    "        trn_losses, val_losses, val_metrics, val_preds_best = run_fold(fold       = fold, \n",
    "                                                                       trn_loader = trn_loader, \n",
    "                                                                       val_loader = val_loader, \n",
    "                                                                       df         = df,\n",
    "                                                                       df_2019    = df_2019 if CFG['data_2019'] else None,\n",
    "                                                                       df_pl      = df_pl if CFG['data_pl'] else None,\n",
    "                                                                       df_ext     = df_ext if CFG['data_ext'] else None,\n",
    "                                                                       df_no      = df_no,\n",
    "                                                                       list_dupl  = list_dupl if CFG['drop_dupl'] else [],\n",
    "                                                                       list_noise = list_noise if CFG['drop_noise'] else [],\n",
    "                                                                       df_trn     = df_trn, \n",
    "                                                                       df_val     = df_val, \n",
    "                                                                       CFG        = CFG, \n",
    "                                                                       device     = device)\n",
    "    else:\n",
    "        def _mp_fn(rank, CFG):\n",
    "            torch.set_default_tensor_type('torch.FloatTensor')\n",
    "            device = xm.xla_device()\n",
    "            trn_losses, val_losses, val_metrics, val_preds_best = run_fold(fold       = fold, \n",
    "                                                                            trn_loader = trn_loader, \n",
    "                                                                            val_loader = val_loader, \n",
    "                                                                            df         = df,\n",
    "                                                                            df_2019    = df_2019 if CFG['data_2019'] else None,\n",
    "                                                                            df_pl      = df_pl if CFG['data_pl'] else None,\n",
    "                                                                            df_ext     = df_ext if CFG['data_ext'] else None,\n",
    "                                                                            df_no      = df_no,\n",
    "                                                                            list_dupl  = list_dupl if CFG['drop_dupl'] else [],\n",
    "                                                                            list_noise = list_noise if CFG['drop_noise'] else [],\n",
    "                                                                            df_trn     = df_trn, \n",
    "                                                                            df_val     = df_val, \n",
    "                                                                            CFG        = CFG, \n",
    "                                                                            device     = device)\n",
    "            if rank == 0:\n",
    "                np.save('trn_losses.npy',     np.array(trn_losses))\n",
    "                np.save('val_losses.npy',     np.array(val_losses))\n",
    "                np.save('val_metrics.npy',    np.array(val_metrics))\n",
    "                np.save('val_preds_best.npy', val_preds_best)\n",
    "        xmp.spawn(_mp_fn, args = (CFG, ), nprocs = CFG['num_workers'], start_method = 'fork')\n",
    "        trn_losses          = np.load('trn_losses.npy')\n",
    "        val_losses          = np.load('val_losses.npy')\n",
    "        val_metrics         = np.load('val_metrics.npy')\n",
    "        val_preds_best      = np.load('val_preds_best.npy')\n",
    "\n",
    "\n",
    "    ### SAVE RESULTS\n",
    "    \n",
    "    # performance table\n",
    "    if CFG['training']:\n",
    "        perf = pd.concat([perf, \n",
    "                          pd.DataFrame({'fold':     [fold] * (CFG['num_epochs'] + CFG['fine_tune']), \n",
    "                                        'epoch':    list(range(CFG['num_epochs'] + CFG['fine_tune'])),\n",
    "                                        'trn_loss': trn_losses,\n",
    "                                        'val_loss': val_losses,\n",
    "                                        'val_acc':  val_metrics})],\n",
    "                        axis = 0)\n",
    "        perf.to_csv(CFG['out_path'] + 'tab_performance.csv', index = False)\n",
    "            \n",
    "    # export OOF predictions\n",
    "    val_preds_df = pd.DataFrame(val_preds_best, columns = ['c' + str(class_idx) for class_idx in range(CFG['num_classes'])])\n",
    "    val_preds_df = pd.concat([df_val, val_preds_df], axis = 1)\n",
    "    oof = pd.concat([oof, val_preds_df], axis = 0).reset_index(drop = True)\n",
    "    oof.to_csv(CFG['out_path'] + 'oof.csv', index = False)\n",
    "\n",
    "\n",
    "    ### DISPLAY FEEDBACK\n",
    "\n",
    "    # feedback\n",
    "    print('-' * 55)\n",
    "    print('Best: acc = {:.4f} (epoch {}), loss = {:.4f} (epoch {})'.format(\n",
    "        np.max(val_metrics), np.argmax(val_metrics) + 1, np.min(val_losses), np.argmin(val_losses) + 1))\n",
    "    print('-' * 55)\n",
    "\n",
    "    # plot loss dynamics\n",
    "    if CFG['training']:\n",
    "        plot_results(trn_losses, val_losses, val_metrics, fold, CFG)\n",
    "\n",
    "\n",
    "##### FEEDBACK\n",
    "\n",
    "print('')\n",
    "print('Finished in {:.2f} minutes'.format((time.time() - cv_start) / 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e5bUh1-_0VjL"
   },
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJQFNQsc0VjL",
    "outputId": "c21e5e9f-1f0d-408a-aa18-ceb4e37c96dc"
   },
   "outputs": [],
   "source": [
    "####### CHECK OOF PERFORMANCE\n",
    "\n",
    "# compute accuracy\n",
    "oof['pred'] = np.argmax(oof[['c' + str(class_idx) for class_idx in range(CFG['num_classes'])]].values, axis = 1)\n",
    "oof_acc     = (oof['pred'] == oof['label']).sum() / len(oof)\n",
    "oof_loss    = perf.groupby('fold')['val_loss'].agg('min').mean()\n",
    "\n",
    "# print results\n",
    "print('- OOF acc  = {:.4f}'.format(oof_acc))\n",
    "print('- OOF loss = {:.4f}'.format(oof_loss))\n",
    "\n",
    "# save results\n",
    "res = pd.DataFrame({'fold':     ['oof',], \n",
    "                    'epoch':    ['best'],\n",
    "                    'trn_loss': [np.nan],\n",
    "                    'val_loss': [oof_loss],\n",
    "                    'val_acc':  [oof_acc]})\n",
    "perf = pd.concat([perf, res], axis = 0)\n",
    "perf.to_csv(CFG['out_path'] + 'tab_performance.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "MWtfNOfkxNZr",
    "outputId": "78c86e7f-bbf6-482e-dc7f-940cbd65e2c8"
   },
   "outputs": [],
   "source": [
    "####### CONFUSION MATRIX\n",
    "\n",
    "# construct confusion matrx\n",
    "cm    = confusion_matrix(y_true = oof['label'], y_pred = oof['pred'], normalize = 'all')\n",
    "annot = np.around(cm, 2)\n",
    "\n",
    "# plot matrix\n",
    "fig, ax = plt.subplots(figsize = (10, 8))\n",
    "sns.heatmap(cm, cmap = 'Blues', annot = annot, lw = 0.5)\n",
    "ax.set_xlabel('Prediction')\n",
    "ax.set_ylabel('Ground Truth')\n",
    "ax.set_aspect('equal')\n",
    "    \n",
    "# export plot\n",
    "plt.savefig(CFG['out_path'] + 'fig_confusion.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g1eeDFgleGB"
   },
   "outputs": [],
   "source": [
    "####### EXPORT CONFIGURATION\n",
    "\n",
    "# store performance values\n",
    "CFG['time_min'] = (time.time() - cv_start) / 60\n",
    "CFG['oof_acc']  = oof_acc\n",
    "CFG['oof_loss'] = oof_loss\n",
    "\n",
    "# save DF\n",
    "params = pd.DataFrame.from_dict(CFG, orient = 'index', columns = ['value'])\n",
    "params.to_csv(CFG['out_path'] + 'tab_configuration.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-9ZKt08y0ViT",
    "T2OuAGJw0Viv",
    "EVog-f8UjJzC",
    "SVfS9wOWAouA",
    "NtAEmeT80VjI"
   ],
   "name": "pytorch-training-v118.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
